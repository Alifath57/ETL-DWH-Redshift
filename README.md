# AWS Data Warehouse

## Introduction
A music streaming startup, Sparkify, an expanded user and song database and want to move it onto the cloud. Their data resides on S3 (AWS storage service) as raw JSON files.

### Project Description
The task is build an ETL pipeline that extracts data from S3 into staging tables in Redshift, and transforms data into into analytics tables following a typical STAR schema to continue finding insights in what songs their users are listening to.

### Song JSON data
The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/).  
Each JSON file has metadata for a particular track, with the following fields:
- num_songs, artist_id, artist_latitude, artist_longitude, artist_location, artist_name, song_id, title, duration, year

### Log JSON data  
The second dataset is user songplay log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim). These simulate activity logs from a music streaming app based on specified configurations.  
An example of the data in 2018-11-12-events.json:  
![](images/log-data.PNG)
 
#### Staging Tables information

**staging_events**

Hold the information from log files, the table contais the columns (artist_name, auth, first_name, gender, item_Session, last_name, length, level, location, method, page, registration, session_id, song_title, status, start_time, user_agent, user_id) for sorting is used the column item_session and the style All.

**staging_songs**

Hold the information from the song files, the table contains the columns (num_songs, artist_id, latitude, longitude, location, artist_name, song_id, title, duration, year) for distribuition and sorting is used sond_id and style all.
### Redshift database schema
**Analytic tables:**

* Fact Table
  * songplays: records in event data associated with song plays.

* Dimension Tables
  * users
  * songs
  * artists
  * time
  
### Files information

- create_table.py create your fact and dimension tables for the star schema in Redshift.
- etl.py is load data from S3 into staging tables on Redshift and then process that data into analytics tables on Redshift.
- sql_queries.py statements, which will be imported into the two other files above.
- dwh.cfg configuration file with the password, keys and information of the Redshift Cluster

### Instructions 
The data from S3 files will be load in the staging tables (staging_events, staging_songs), the information is processed and load in the analytics tables (songplays, users, songs, artists, time)

1. Look through **sql_queries.py** to understand what SQL queries are used throughout the Python scripts.   
2. Run **create_tables.py** in terminal to refresh and create the tables in Redshift
3. Run **etl.py** to load the data from S3 to Redshift, and populate the analytics tables
